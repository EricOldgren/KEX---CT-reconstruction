{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"WebAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "GIT_ROOT = Path(\"../..\").resolve()\n",
    "SRC = GIT_ROOT / \"src\"\n",
    "if not SRC in sys.path:\n",
    "    sys.path.append(str(SRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 cuda:0 torch.Size([30, 720, 560])\n",
      "Epoch: 1 loss is: 0.4657011207818689 sino loss is: 2.1416247606277468 recon loss is: 0.2515386429309549\n",
      "Epoch: 2 loss is: 0.1593964686810355 sino loss is: 0.6558303952217102 recon loss is: 0.09381342662566708\n",
      "Epoch: 3 loss is: 0.13253633472443388 sino loss is: 0.5159070253372192 recon loss is: 0.08094563055158423\n",
      "Epoch: 4 loss is: 0.1293597278490482 sino loss is: 0.44957255125045775 recon loss is: 0.08440247302202564\n",
      "Epoch: 5 loss is: 0.14283618812799875 sino loss is: 0.5406239271163941 recon loss is: 0.08877379407525483\n",
      "Epoch: 6 loss is: 0.1274639356436881 sino loss is: 0.4717994570732117 recon loss is: 0.08028398859526245\n",
      "Epoch: 7 loss is: 0.11829293390982669 sino loss is: 0.43819177746772764 recon loss is: 0.07447375452392618\n",
      "Epoch: 8 loss is: 0.11057496912561542 sino loss is: 0.4163780093193054 recon loss is: 0.06893716700159198\n",
      "Epoch: 9 loss is: 0.10588835503070378 sino loss is: 0.4045316517353058 recon loss is: 0.06543518963365579\n",
      "Epoch: 10 loss is: 0.10583756112338345 sino loss is: 0.40906464457511904 recon loss is: 0.06493109606982511\n",
      "Epoch: 11 loss is: 0.10878690868488076 sino loss is: 0.39159883856773375 recon loss is: 0.06962702363601449\n",
      "Epoch: 12 loss is: 0.1029335166484728 sino loss is: 0.37514299154281616 recon loss is: 0.06541921712166215\n",
      "Epoch: 13 loss is: 0.11745602583014886 sino loss is: 0.37945919036865233 recon loss is: 0.07951010597371976\n",
      "Epoch: 14 loss is: 0.11033445524076596 sino loss is: 0.5089742839336395 recon loss is: 0.05943702610234396\n",
      "Epoch: 15 loss is: 0.10510551547087214 sino loss is: 0.44052419662475584 recon loss is: 0.06105309454179786\n",
      "Epoch: 16 loss is: 0.0961132698910354 sino loss is: 0.3835290253162384 recon loss is: 0.05776036691237673\n",
      "Epoch: 17 loss is: 0.09731963811012041 sino loss is: 0.3654307186603546 recon loss is: 0.060776565722544314\n",
      "Epoch: 18 loss is: 0.09086240363279943 sino loss is: 0.35712855458259585 recon loss is: 0.055149547951022435\n",
      "Epoch: 19 loss is: 0.08620941533358557 sino loss is: 0.3542838513851166 recon loss is: 0.05078102915199264\n",
      "Epoch: 20 loss is: 0.08513543597774523 sino loss is: 0.3402402102947235 recon loss is: 0.05111141457574385\n",
      "Epoch: 21 loss is: 0.08416142483623025 sino loss is: 0.3291356325149536 recon loss is: 0.05124786143572328\n",
      "Epoch: 22 loss is: 0.08547682042870573 sino loss is: 0.31468449234962464 recon loss is: 0.05400837015066198\n",
      "Epoch: 23 loss is: 0.09310465192119062 sino loss is: 0.3448303371667862 recon loss is: 0.05862161738494814\n",
      "Epoch: 24 loss is: 0.0827316060465496 sino loss is: 0.3070481061935425 recon loss is: 0.05202679408609085\n",
      "Epoch: 25 loss is: 0.07881681318683655 sino loss is: 0.3040238320827484 recon loss is: 0.04841442923350364\n",
      "Epoch: 26 loss is: 0.08033840860656692 sino loss is: 0.295650315284729 recon loss is: 0.05077337640754177\n",
      "Epoch: 27 loss is: 0.15128783883998345 sino loss is: 0.3485622048377991 recon loss is: 0.11643161776015709\n",
      "Epoch: 28 loss is: 0.15029526814473573 sino loss is: 0.3705617725849152 recon loss is: 0.11323909103525581\n",
      "Epoch: 29 loss is: 0.12457355683887876 sino loss is: 0.47264657616615297 recon loss is: 0.0773088985517112\n",
      "Epoch: 30 loss is: 0.110162962289842 sino loss is: 0.4515560805797577 recon loss is: 0.06500735400834881\n",
      "Epoch: 31 loss is: 0.09701626585083871 sino loss is: 0.37804684042930603 recon loss is: 0.05921158143537908\n",
      "Epoch: 32 loss is: 0.09445234770269523 sino loss is: 0.37191646695137026 recon loss is: 0.05726070041151176\n",
      "Epoch: 33 loss is: 0.09722833394584397 sino loss is: 0.3155458629131317 recon loss is: 0.06567374676046113\n",
      "Epoch: 34 loss is: 0.11288858665700503 sino loss is: 0.3681792438030243 recon loss is: 0.07607066160615035\n",
      "Epoch: 35 loss is: 0.10447246252381767 sino loss is: 0.3351463556289673 recon loss is: 0.0709578265138861\n",
      "Epoch: 36 loss is: 0.12396443336409117 sino loss is: 0.27110216915607455 recon loss is: 0.09685421585243728\n",
      "Epoch: 37 loss is: 0.10337946875297682 sino loss is: 0.35501055121421815 recon loss is: 0.06787841333353178\n",
      "Epoch: 38 loss is: 0.10020354862336249 sino loss is: 0.3793273150920868 recon loss is: 0.06227081629458994\n",
      "Epoch: 39 loss is: 0.09187400052460816 sino loss is: 0.3576219975948334 recon loss is: 0.05611180002006677\n",
      "Epoch: 40 loss is: 0.08835030897574597 sino loss is: 0.34627952575683596 recon loss is: 0.05372235580401594\n",
      "Epoch: 41 loss is: 0.085281044776041 sino loss is: 0.31106157004833224 recon loss is: 0.0541748868026323\n",
      "Epoch: 42 loss is: 0.08336460398967493 sino loss is: 0.3102453947067261 recon loss is: 0.05234006377394427\n",
      "Epoch: 43 loss is: 0.08109036285428078 sino loss is: 0.29571251273155214 recon loss is: 0.051519110910573325\n",
      "Epoch: 44 loss is: 0.07958381923672109 sino loss is: 0.29741880893707273 recon loss is: 0.04984193744894415\n",
      "Epoch: 45 loss is: 0.0792636895940211 sino loss is: 0.28494701981544496 recon loss is: 0.050768986941924346\n",
      "Epoch: 46 loss is: 0.08287704193488402 sino loss is: 0.27586170732975007 recon loss is: 0.05529087127641481\n",
      "Epoch: 47 loss is: 0.11653386442006784 sino loss is: 0.2871019750833511 recon loss is: 0.08782366601766306\n",
      "Epoch: 48 loss is: 0.09941446419365649 sino loss is: 0.29185892939567565 recon loss is: 0.07022857080705408\n",
      "Epoch: 49 loss is: 0.09961807879874857 sino loss is: 0.27061104476451875 recon loss is: 0.07255697402427347\n",
      "Epoch: 50 loss is: 0.09406625843076698 sino loss is: 0.3242589354515076 recon loss is: 0.06164036421506398\n",
      "Epoch: 51 loss is: 0.0900392504636344 sino loss is: 0.2971480369567871 recon loss is: 0.06032444594839183\n",
      "Epoch: 52 loss is: 0.0848798548584504 sino loss is: 0.3086200177669525 recon loss is: 0.0540178524112029\n",
      "Epoch: 53 loss is: 0.0790402414896016 sino loss is: 0.3025092422962189 recon loss is: 0.04878931740899133\n",
      "Epoch: 54 loss is: 0.07882537319092768 sino loss is: 0.27502394318580625 recon loss is: 0.05132297835080641\n",
      "Epoch: 55 loss is: 0.0738191213202994 sino loss is: 0.2777192026376724 recon loss is: 0.046047200236968296\n",
      "Epoch: 56 loss is: 0.0730247552501221 sino loss is: 0.2619451403617859 recon loss is: 0.046830241288449315\n",
      "Epoch: 57 loss is: 0.07218986676943434 sino loss is: 0.26666441559791565 recon loss is: 0.04552342483711375\n",
      "Epoch: 58 loss is: 0.0709141026353311 sino loss is: 0.2627507925033569 recon loss is: 0.04463902293796057\n",
      "Epoch: 59 loss is: 0.07202702423243239 sino loss is: 0.264876589179039 recon loss is: 0.04553936456947043\n",
      "Epoch: 60 loss is: 0.07098669251317256 sino loss is: 0.2559096574783325 recon loss is: 0.0453957266163277\n",
      "Epoch: 61 loss is: 0.06965497814316421 sino loss is: 0.26160222887992857 recon loss is: 0.04349475436110167\n",
      "Epoch: 62 loss is: 0.06910977878701541 sino loss is: 0.24741770327091217 recon loss is: 0.04436800771486613\n",
      "Epoch: 63 loss is: 0.06938638288675475 sino loss is: 0.2551622152328491 recon loss is: 0.0438701608419292\n",
      "Epoch: 64 loss is: 0.06833270157635325 sino loss is: 0.24571408033370973 recon loss is: 0.04376129324495906\n",
      "Epoch: 65 loss is: 0.06886833489248835 sino loss is: 0.24807618856430053 recon loss is: 0.04406071596155249\n",
      "Epoch: 66 loss is: 0.06955946356274512 sino loss is: 0.244907945394516 recon loss is: 0.045068668278235455\n",
      "Epoch: 67 loss is: 0.077304017170072 sino loss is: 0.25308387279510497 recon loss is: 0.05199562944352667\n",
      "Epoch: 68 loss is: 0.07242830088934463 sino loss is: 0.242098468542099 recon loss is: 0.0482184535880999\n",
      "Epoch: 69 loss is: 0.06862981497092312 sino loss is: 0.24894848167896272 recon loss is: 0.04373496650500363\n",
      "Epoch: 70 loss is: 0.06633051148810433 sino loss is: 0.24119286835193635 recon loss is: 0.04221122413137005\n",
      "Epoch: 71 loss is: 0.06893995393417124 sino loss is: 0.24665502607822418 recon loss is: 0.04427445073030238\n",
      "Epoch: 72 loss is: 0.07306277163076214 sino loss is: 0.24203872084617614 recon loss is: 0.04885889895009808\n",
      "Epoch: 73 loss is: 0.07510201813486689 sino loss is: 0.24958307445049285 recon loss is: 0.050143709721242116\n",
      "Epoch: 74 loss is: 0.07406337937912068 sino loss is: 0.24793062806129457 recon loss is: 0.04927031664749703\n",
      "Epoch: 75 loss is: 0.07535922670539762 sino loss is: 0.2440871477127075 recon loss is: 0.05095051148709202\n",
      "Epoch: 76 loss is: 0.06837973637435794 sino loss is: 0.24001526832580566 recon loss is: 0.04437820879671932\n",
      "Epoch: 77 loss is: 0.06722801701340128 sino loss is: 0.24539670050144197 recon loss is: 0.042688346665233856\n",
      "Epoch: 78 loss is: 0.06862898662969698 sino loss is: 0.2458899885416031 recon loss is: 0.04403998747751344\n",
      "Epoch: 79 loss is: 0.0702205153299699 sino loss is: 0.2468835860490799 recon loss is: 0.04553215635253287\n",
      "Epoch: 80 loss is: 0.07277102100263677 sino loss is: 0.23677778840065003 recon loss is: 0.04909324201356015\n",
      "Epoch: 81 loss is: 0.06569708206615599 sino loss is: 0.24070631563663483 recon loss is: 0.041626450726009914\n",
      "Epoch: 82 loss is: 0.06597402038261226 sino loss is: 0.234469273686409 recon loss is: 0.04252709241792491\n",
      "Epoch: 83 loss is: 0.07089414561852896 sino loss is: 0.2447180688381195 recon loss is: 0.04642233813867056\n",
      "Epoch: 84 loss is: 0.06882754742262208 sino loss is: 0.23546580076217652 recon loss is: 0.045280967197392816\n",
      "Epoch: 85 loss is: 0.06986461531183501 sino loss is: 0.23704993426799775 recon loss is: 0.04615962158701201\n",
      "Epoch: 86 loss is: 0.08110918423329529 sino loss is: 0.23985198140144348 recon loss is: 0.05712398572062191\n",
      "Epoch: 87 loss is: 0.07877880780263265 sino loss is: 0.24067001640796662 recon loss is: 0.05471180564029535\n",
      "Epoch: 88 loss is: 0.08283500154418393 sino loss is: 0.2260839283466339 recon loss is: 0.060226608113474085\n",
      "Epoch: 89 loss is: 0.07339582315670007 sino loss is: 0.2494066148996353 recon loss is: 0.04845516144321912\n",
      "Epoch: 90 loss is: 0.06656496201722595 sino loss is: 0.24824893474578857 recon loss is: 0.041740068170118064\n",
      "Epoch: 91 loss is: 0.06476795139748151 sino loss is: 0.24232951998710633 recon loss is: 0.04053499910074765\n",
      "Epoch: 92 loss is: 0.06427800474359287 sino loss is: 0.23328160345554352 recon loss is: 0.04094984387649788\n",
      "Epoch: 93 loss is: 0.06481877278601003 sino loss is: 0.23423583507537843 recon loss is: 0.04139518898044898\n",
      "Epoch: 94 loss is: 0.06366059842654254 sino loss is: 0.2282903015613556 recon loss is: 0.040831567972383756\n",
      "Epoch: 95 loss is: 0.06301953283257422 sino loss is: 0.22833435535430907 recon loss is: 0.04018609685010847\n",
      "Epoch: 96 loss is: 0.06288330228361064 sino loss is: 0.22425818741321563 recon loss is: 0.04045748309525424\n",
      "Epoch: 97 loss is: 0.06352131552210705 sino loss is: 0.22713879942893983 recon loss is: 0.040807435281189845\n",
      "Epoch: 98 loss is: 0.06209589234257736 sino loss is: 0.22102792263031007 recon loss is: 0.039993099781523135\n",
      "Epoch: 99 loss is: 0.0621103394822616 sino loss is: 0.2211148053407669 recon loss is: 0.039998858426644274\n",
      "Epoch: 100 loss is: 0.06292379176204771 sino loss is: 0.21981803476810455 recon loss is: 0.040941988061719836\n"
     ]
    }
   ],
   "source": [
    "from utils.data import get_htc2022_train_phantoms, get_kits_train_phantoms, get_htclike_train_phantoms\n",
    "from utils.polynomials import Legendre, Chebyshev\n",
    "from geometries import FlatFanBeamGeometry, DEVICE, HTC2022_GEOMETRY, ParallelGeometry\n",
    "from geometries.geometry_base import naive_sino_filling, mark_cyclic\n",
    "from models.fbps import AdaptiveFBP as AFBP\n",
    "from models.FNOBPs.fnobp import FNO_BP\n",
    "from models.SerieBPs.series_bp1 import Series_BP\n",
    "from models.modelbase import plot_model_progress\n",
    "from statistics import mean\n",
    "\n",
    "ar = 0.25 #angle ratio of full 360 deg scan\n",
    "PHANTOM_DATA = torch.concat([get_htc2022_train_phantoms(), get_htclike_train_phantoms()])\n",
    "geometry = HTC2022_GEOMETRY\n",
    "# PHANTOM_DATA = get_kits_train_phantoms()\n",
    "# geometry = FlatFanBeamGeometry(720, 560, 410.66, 543.74, 112, [-40,40, -40, 40], [256, 256])\n",
    "# geometry = FlatFanBeamGeometry(1800, 300, 1.5, 3.0, 4.0, [-1,1,-1,1], [256, 256])\n",
    "# geometry = ParallelGeometry(1800, 300, [-1,1,-1,1], [256, 256])\n",
    "N_known_angles = int(geometry.n_projections*ar)\n",
    "N_angles_out = int(geometry.n_projections*0.6) #can be 0.5 if parallel beam\n",
    "SINO_DATA = geometry.project_forward(PHANTOM_DATA)\n",
    "print(SINO_DATA.dtype, SINO_DATA.device, SINO_DATA.shape)\n",
    "\n",
    "# model = AFBP(geometry)\n",
    "# model = FNO_BP(geometry, hidden_layers=[40,40], n_known_angles=N_known_angles, n_angles_out=N_angles_out)\n",
    "model = Series_BP(geometry, ar, 120, 60, Legendre.key)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(SINO_DATA, PHANTOM_DATA)\n",
    "dataloader = DataLoader(dataset, batch_size=6, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "mse_fn = lambda diff : torch.mean(diff**2)\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses, batch_sino_losses, batch_recon_losses = [], [], []\n",
    "    for sino_batch, phantom_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_ind = 0 #torch.randint(0, geometry.n_projections, (1,)).item()\n",
    "        known_beta_bool = torch.zeros(geometry.n_projections, device=DEVICE, dtype=bool)\n",
    "        out_beta_bool = known_beta_bool.clone()\n",
    "        mark_cyclic(known_beta_bool, start_ind, (start_ind+N_known_angles)%geometry.n_projections)#known_beta_bool is True at angles where sinogram is meassured and false otherwise\n",
    "        mark_cyclic(out_beta_bool, start_ind, (start_ind+N_angles_out)%geometry.n_projections)\n",
    "        la_sinos = sino_batch * 0 #limited angle sinograms\n",
    "        la_sinos[:, known_beta_bool] = sino_batch[:, known_beta_bool]\n",
    "\n",
    "        filtered = model.get_extrapolated_filtered_sinos(la_sinos, known_beta_bool, out_beta_bool)\n",
    "        gt_filtered = geometry.inverse_fourier_transform(geometry.fourier_transform(sino_batch*geometry.jacobian_det)*geometry.ram_lak_filter())\n",
    "        loss_sino_domain = mse_fn(gt_filtered-filtered)\n",
    "\n",
    "        recons = F.relu(geometry.project_backward(filtered/2)) #sinogram covers 360deg  - double coverage\n",
    "        loss_recon_domain = mse_fn(phantom_batch - recons)\n",
    "\n",
    "        loss = loss_recon_domain + loss_sino_domain*0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().item())\n",
    "        batch_sino_losses.append(loss_sino_domain.cpu().item())\n",
    "        batch_recon_losses.append(loss_recon_domain.cpu().item())\n",
    "    \n",
    "    print(\"Epoch:\", epoch+1, \"loss is:\", mean(batch_losses), \"sino loss is:\", mean(batch_sino_losses), \"recon loss is:\", mean(batch_recon_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "FNO_BP\n",
      "sinogram mse: tensor(1652.9601, device='cuda:0')\n",
      "filterd sinogram mse:  tensor(2180.3899, device='cuda:0')\n",
      "reconstruction mse:  tensor(0.0402, device='cuda:0', dtype=torch.float64)\n",
      "========================================\n",
      "FBP\n",
      "sinogram mse: tensor(1099.7948, device='cuda:0')\n",
      "filterd sinogram mse:  tensor(2194.1360, device='cuda:0')\n",
      "reconstruction mse:  tensor(0.1856, device='cuda:0', dtype=torch.float64)\n",
      "Press Ctrl+C to stop WebAgg server\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m plot_model_progress(model, SINO_DATA[:\u001b[39m5\u001b[39m], known_angles, out_angles, PHANTOM_DATA[:\u001b[39m5\u001b[39m], disp_ind\u001b[39m=\u001b[39mdisp_ind)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m plot_model_progress(fbp, SINO_DATA[:\u001b[39m5\u001b[39m], known_angles, out_angles, PHANTOM_DATA[:\u001b[39m5\u001b[39m], disp_ind\u001b[39m=\u001b[39mdisp_ind)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m plt\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/matplotlib/pyplot.py:368\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mexplicitly there.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    367\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 368\u001b[0m \u001b[39mreturn\u001b[39;00m _backend_mod\u001b[39m.\u001b[39;49mshow(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/matplotlib/backends/backend_webagg.py:320\u001b[0m, in \u001b[0;36m_BackendWebAgg.show\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTo view figure, visit \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(url))\n\u001b[0;32m--> 320\u001b[0m WebAggApplication\u001b[39m.\u001b[39;49mstart()\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/matplotlib/backends/backend_webagg.py:277\u001b[0m, in \u001b[0;36mWebAggApplication.start\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    275\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    276\u001b[0m \u001b[39mwith\u001b[39;00m catch_sigint():\n\u001b[0;32m--> 277\u001b[0m     ioloop\u001b[39m.\u001b[39;49mstart()\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/tornado/platform/asyncio.py:199\u001b[0m, in \u001b[0;36mBaseAsyncIOLoop.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_logging()\n\u001b[1;32m    198\u001b[0m     asyncio\u001b[39m.\u001b[39mset_event_loop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39masyncio_loop)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masyncio_loop\u001b[39m.\u001b[39;49mrun_forever()\n\u001b[1;32m    200\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     asyncio\u001b[39m.\u001b[39mset_event_loop(old_loop)\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/asyncio/base_events.py:586\u001b[0m, in \u001b[0;36mBaseEventLoop.run_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39m\"\"\"Run until stop() is called.\"\"\"\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m--> 586\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_running()\n\u001b[1;32m    587\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_coroutine_origin_tracking(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_debug)\n\u001b[1;32m    588\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_thread_id \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mget_ident()\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/asyncio/base_events.py:578\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_running\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    577\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_running():\n\u001b[0;32m--> 578\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThis event loop is already running\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    579\u001b[0m     \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    581\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mCannot run the event loop while another loop is running\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "from models.fbps import FBP\n",
    "fbp = FBP(model.geometry)\n",
    "#Clear previous plots\n",
    "for i in plt.get_fignums():\n",
    "    plt.figure(i)\n",
    "    plt.close()\n",
    "\n",
    "zero_cropped_sinos, known_region = geometry.reflect_fill_sinos(*geometry.zero_cropp_sinos(SINO_DATA[:5], ar, 0))\n",
    "angles = torch.zeros(geometry.n_projections, dtype=bool, device=DEVICE)\n",
    "known_angles = mark_cyclic(angles.clone(), 0, N_known_angles)\n",
    "out_angles = mark_cyclic(angles.clone(), 0, N_angles_out)\n",
    "# zero_cropped_sinos = naive_sino_filling(zero_cropped_sinos, (~known_region).sum(dim=-1) == 0)\n",
    "disp_ind = 2\n",
    "plot_model_progress(model, SINO_DATA[:5], known_angles, out_angles, PHANTOM_DATA[:5], disp_ind=disp_ind)\n",
    "plot_model_progress(fbp, SINO_DATA[:5], known_angles, out_angles, PHANTOM_DATA[:5], disp_ind=disp_ind)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full angle fbp recons mse\n",
    "recons = geometry.fbp_reconstruct(SINO_DATA)\n",
    "print(torch.mean((recons-PHANTOM_DATA)**2).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to /home/emastr/deep-limited-angle/KEX---CT-reconstruction/data/models/fno_bp_fanbeamkits_ar0.25.pt\n"
     ]
    }
   ],
   "source": [
    "#Save model\n",
    "modelname = \"fno_bp_fanbeamkits_ar0.25\"\n",
    "from models.modelbase import save_model_checkpoint\n",
    "save_path = GIT_ROOT / \"data\" / \"models\" / (modelname + \".pt\")\n",
    "save_model_checkpoint(model, optimizer, loss, ar, save_path)\n",
    "print(\"model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odl_torch",
   "language": "python",
   "name": "odl_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
