{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"WebAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "GIT_ROOT = Path(\"../..\").resolve()\n",
    "SRC = GIT_ROOT / \"src\"\n",
    "if not SRC in sys.path:\n",
    "    sys.path.append(str(SRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 cuda:0 torch.Size([30, 720, 560])\n",
      "Epoch: 1 loss is: 0.7661406057924995 sino loss is: 2.268755567073822 recon loss is: 0.5392650495321522\n",
      "Epoch: 2 loss is: 0.6174284484641824 sino loss is: 0.5585854411125183 recon loss is: 0.5615699027138029\n",
      "Epoch: 3 loss is: 0.6158038087022187 sino loss is: 0.5417538046836853 recon loss is: 0.5616284273397806\n",
      "Epoch: 4 loss is: 0.6146564530127546 sino loss is: 0.5265403389930725 recon loss is: 0.5620024191134474\n",
      "Epoch: 5 loss is: 0.6140012047689866 sino loss is: 0.5207212805747986 recon loss is: 0.5619290761154603\n",
      "Epoch: 6 loss is: 0.6135230691885885 sino loss is: 0.5191152572631836 recon loss is: 0.5616115422701772\n",
      "Epoch: 7 loss is: 0.6131209582774032 sino loss is: 0.5171555995941162 recon loss is: 0.5614053972749103\n",
      "Epoch: 8 loss is: 0.6127577460983262 sino loss is: 0.5121966660022735 recon loss is: 0.5615380793490873\n",
      "Epoch: 9 loss is: 0.6124168724460384 sino loss is: 0.5082147896289826 recon loss is: 0.5615953922910472\n",
      "Epoch: 10 loss is: 0.6120859199576126 sino loss is: 0.506048184633255 recon loss is: 0.5614811004512059\n",
      "Epoch: 11 loss is: 0.6117865077603752 sino loss is: 0.5047756254673004 recon loss is: 0.5613089441705639\n",
      "Epoch: 12 loss is: 0.6115080258078013 sino loss is: 0.5036890923976898 recon loss is: 0.561139114779893\n",
      "Epoch: 13 loss is: 0.6112415085083951 sino loss is: 0.5036843776702881 recon loss is: 0.5608730691022386\n",
      "Epoch: 14 loss is: 0.6109659725073299 sino loss is: 0.5025630354881286 recon loss is: 0.5607096688095053\n",
      "Epoch: 15 loss is: 0.6106916227606625 sino loss is: 0.5019005596637726 recon loss is: 0.5605015659002156\n",
      "Epoch: 16 loss is: 0.6104242370126551 sino loss is: 0.5024251997470855 recon loss is: 0.5601817153988189\n",
      "Epoch: 17 loss is: 0.6101624213667806 sino loss is: 0.5027946531772614 recon loss is: 0.5598829553039963\n",
      "Epoch: 18 loss is: 0.6098998699631059 sino loss is: 0.5013202905654908 recon loss is: 0.5597678404595219\n",
      "Epoch: 19 loss is: 0.6096321067903975 sino loss is: 0.5020575940608978 recon loss is: 0.5594263470862845\n",
      "Epoch: 20 loss is: 0.6093595174156025 sino loss is: 0.5020312249660492 recon loss is: 0.5591563944719627\n",
      "Epoch: 21 loss is: 0.6090999613473628 sino loss is: 0.5026489555835724 recon loss is: 0.5588350648949358\n",
      "Epoch: 22 loss is: 0.6088187200180305 sino loss is: 0.5028837978839874 recon loss is: 0.5585303393355621\n",
      "Epoch: 23 loss is: 0.6085330327853081 sino loss is: 0.5037613570690155 recon loss is: 0.558156896929395\n",
      "Epoch: 24 loss is: 0.6082434968906483 sino loss is: 0.5046949207782745 recon loss is: 0.5577740037697396\n",
      "Epoch: 25 loss is: 0.6079487889323599 sino loss is: 0.5055252134799957 recon loss is: 0.557396266541279\n",
      "Epoch: 26 loss is: 0.6076442661660516 sino loss is: 0.5055854022502899 recon loss is: 0.5570857251959646\n",
      "Epoch: 27 loss is: 0.6073373801530265 sino loss is: 0.508260303735733 recon loss is: 0.5565113491834067\n",
      "Epoch: 28 loss is: 0.60699655692641 sino loss is: 0.5081374168395996 recon loss is: 0.5561828149444268\n",
      "Epoch: 29 loss is: 0.6066561965845425 sino loss is: 0.5107832074165344 recon loss is: 0.5555778755448658\n",
      "Epoch: 30 loss is: 0.6062943385192313 sino loss is: 0.5119427025318146 recon loss is: 0.5551000678190151\n",
      "Epoch: 31 loss is: 0.6059167431583269 sino loss is: 0.5141584038734436 recon loss is: 0.5545009018769129\n",
      "Epoch: 32 loss is: 0.6055139939750219 sino loss is: 0.5166813194751739 recon loss is: 0.5538458611334348\n",
      "Epoch: 33 loss is: 0.6050772457767915 sino loss is: 0.5187635898590088 recon loss is: 0.5532008870889138\n",
      "Epoch: 34 loss is: 0.6046219651194777 sino loss is: 0.5210441648960114 recon loss is: 0.5525175478848185\n",
      "Epoch: 35 loss is: 0.6041335738333758 sino loss is: 0.5247326612472534 recon loss is: 0.5516603074106273\n",
      "Epoch: 36 loss is: 0.6036085150360576 sino loss is: 0.5280288875102996 recon loss is: 0.5508056259870044\n",
      "Epoch: 37 loss is: 0.6030596819862402 sino loss is: 0.5321547269821167 recon loss is: 0.5498442073508776\n",
      "Epoch: 38 loss is: 0.6024464160864343 sino loss is: 0.534901213645935 recon loss is: 0.5489562944238177\n",
      "Epoch: 39 loss is: 0.6018234488675308 sino loss is: 0.5415265321731567 recon loss is: 0.5476707947561454\n",
      "Epoch: 40 loss is: 0.6011262685191948 sino loss is: 0.546112322807312 recon loss is: 0.5465150363874752\n",
      "Epoch: 41 loss is: 0.600363930270096 sino loss is: 0.550248634815216 recon loss is: 0.5453390657454932\n",
      "Epoch: 42 loss is: 0.5995574576163867 sino loss is: 0.5571508884429932 recon loss is: 0.5438423675799945\n",
      "Epoch: 43 loss is: 0.5986662184428905 sino loss is: 0.5663014650344849 recon loss is: 0.5420360704493259\n",
      "Epoch: 44 loss is: 0.5977107193364997 sino loss is: 0.5707514524459839 recon loss is: 0.5406355727507968\n",
      "Epoch: 45 loss is: 0.5967024621655564 sino loss is: 0.5799816489219666 recon loss is: 0.5387042966773133\n",
      "Epoch: 46 loss is: 0.5955905933718568 sino loss is: 0.5885609149932861 recon loss is: 0.5367345002334004\n",
      "Epoch: 47 loss is: 0.5943678118822288 sino loss is: 0.5977304100990295 recon loss is: 0.5345947705743026\n",
      "Epoch: 48 loss is: 0.5931446681802568 sino loss is: 0.6085317611694336 recon loss is: 0.5322914893811044\n",
      "Epoch: 49 loss is: 0.5918180521382976 sino loss is: 0.6189249157905579 recon loss is: 0.5299255583240677\n",
      "Epoch: 50 loss is: 0.590396797445465 sino loss is: 0.6288244247436523 recon loss is: 0.5275143546730766\n",
      "Epoch: 51 loss is: 0.5889552316420965 sino loss is: 0.6404868364334106 recon loss is: 0.5249065472536973\n",
      "Epoch: 52 loss is: 0.5874020939613634 sino loss is: 0.652711021900177 recon loss is: 0.5221309905792529\n",
      "Epoch: 53 loss is: 0.5858417715849542 sino loss is: 0.6641194343566894 recon loss is: 0.5194298248710297\n",
      "Epoch: 54 loss is: 0.5842312439438787 sino loss is: 0.6784062266349793 recon loss is: 0.5163906185981718\n",
      "Epoch: 55 loss is: 0.5826340067828454 sino loss is: 0.6882025122642517 recon loss is: 0.5138137537682809\n",
      "Epoch: 56 loss is: 0.581000363184143 sino loss is: 0.7009971976280213 recon loss is: 0.5109006428252945\n",
      "Epoch: 57 loss is: 0.5794156902806122 sino loss is: 0.7107141137123107 recon loss is: 0.5083442771212419\n",
      "Epoch: 58 loss is: 0.5777449568924767 sino loss is: 0.7241320490837098 recon loss is: 0.5053317507920129\n",
      "Epoch: 59 loss is: 0.5761008164490797 sino loss is: 0.7361260414123535 recon loss is: 0.5024882114137746\n",
      "Epoch: 60 loss is: 0.574540793262251 sino loss is: 0.745331084728241 recon loss is: 0.5000076824052411\n",
      "Epoch: 61 loss is: 0.5729157083537396 sino loss is: 0.7583038449287415 recon loss is: 0.4970853232648191\n",
      "Epoch: 62 loss is: 0.5713110114153546 sino loss is: 0.7699932098388672 recon loss is: 0.49431168834530526\n",
      "Epoch: 63 loss is: 0.5696872316491131 sino loss is: 0.780418312549591 recon loss is: 0.49164539950008435\n",
      "Epoch: 64 loss is: 0.567978874990641 sino loss is: 0.7864837288856507 recon loss is: 0.48933049852579724\n",
      "Epoch: 65 loss is: 0.5663257960515549 sino loss is: 0.8033012747764587 recon loss is: 0.48599566708379294\n",
      "Epoch: 66 loss is: 0.5645690769117144 sino loss is: 0.8107461094856262 recon loss is: 0.4834944635789661\n",
      "Epoch: 67 loss is: 0.5628441584073337 sino loss is: 0.8256867885589599 recon loss is: 0.4802754786573681\n",
      "Epoch: 68 loss is: 0.5609939773288691 sino loss is: 0.8315760850906372 recon loss is: 0.4778363664356196\n",
      "Epoch: 69 loss is: 0.5590621514501148 sino loss is: 0.8486860990524292 recon loss is: 0.4741935385646397\n",
      "Epoch: 70 loss is: 0.5570330300330639 sino loss is: 0.8582440376281738 recon loss is: 0.4712086238860608\n",
      "Epoch: 71 loss is: 0.5548341832251497 sino loss is: 0.8667467713356019 recon loss is: 0.46815950340938045\n",
      "Epoch: 72 loss is: 0.5526014355562708 sino loss is: 0.8974941968917847 recon loss is: 0.4628520158670924\n",
      "Epoch: 73 loss is: 0.5498101343107407 sino loss is: 0.8861975312232971 recon loss is: 0.46119037910224836\n",
      "Epoch: 74 loss is: 0.5472129639836723 sino loss is: 0.9243213891983032 recon loss is: 0.4547808226796562\n",
      "Epoch: 75 loss is: 0.543893462767159 sino loss is: 0.9325986981391907 recon loss is: 0.4506335932512632\n",
      "Epoch: 76 loss is: 0.5402097259388849 sino loss is: 0.9547351360321045 recon loss is: 0.44473621144160486\n",
      "Epoch: 77 loss is: 0.5358538431456814 sino loss is: 0.9605986595153808 recon loss is: 0.43979397391588787\n",
      "Epoch: 78 loss is: 0.5313070055220914 sino loss is: 1.0024675607681275 recon loss is: 0.4310602482531858\n",
      "Epoch: 79 loss is: 0.5256777539301816 sino loss is: 1.0283464550971986 recon loss is: 0.422843107526392\n",
      "Epoch: 80 loss is: 0.5177628577438131 sino loss is: 1.0370763540267944 recon loss is: 0.4140552208510176\n",
      "Epoch: 81 loss is: 0.5103772824099293 sino loss is: 1.1198534488677978 recon loss is: 0.3983919357350102\n",
      "Epoch: 82 loss is: 0.4967210022224204 sino loss is: 1.0867186546325684 recon loss is: 0.38804913437497773\n",
      "Epoch: 83 loss is: 0.4939245223073811 sino loss is: 1.270904278755188 recon loss is: 0.36683409502790876\n",
      "Epoch: 84 loss is: 0.47393907244408606 sino loss is: 1.0883821964263916 recon loss is: 0.36510084952319144\n",
      "Epoch: 85 loss is: 0.47148428530719005 sino loss is: 1.386545515060425 recon loss is: 0.3328297299268457\n",
      "Epoch: 86 loss is: 0.44857719485644837 sino loss is: 1.1515971422195435 recon loss is: 0.3334174776542618\n",
      "Epoch: 87 loss is: 0.428569994226262 sino loss is: 1.219100260734558 recon loss is: 0.3066599645765275\n",
      "Epoch: 88 loss is: 0.46533783035247794 sino loss is: 1.6516342401504516 recon loss is: 0.3001744057413863\n",
      "Epoch: 89 loss is: 0.4150474703282047 sino loss is: 1.0139397740364076 recon loss is: 0.3136534920304943\n",
      "Epoch: 90 loss is: 0.39401713193088955 sino loss is: 1.2803168296813965 recon loss is: 0.26598544598251767\n",
      "Epoch: 91 loss is: 0.3763306729872451 sino loss is: 1.2632274627685547 recon loss is: 0.25000792522027354\n",
      "Epoch: 92 loss is: 0.34599467548042834 sino loss is: 0.9757985234260559 recon loss is: 0.24841482075363694\n",
      "Epoch: 93 loss is: 0.3432399027883122 sino loss is: 1.1863361120223999 recon loss is: 0.224606291288049\n",
      "Epoch: 94 loss is: 0.3618301833464377 sino loss is: 1.361548948287964 recon loss is: 0.22567528583543228\n",
      "Epoch: 95 loss is: 0.32320840788209065 sino loss is: 0.9830371379852295 recon loss is: 0.224904690507289\n",
      "Epoch: 96 loss is: 0.3267092470791642 sino loss is: 1.302536380290985 recon loss is: 0.1964556087520425\n",
      "Epoch: 97 loss is: 0.33195188208823895 sino loss is: 1.2578954339027404 recon loss is: 0.20616233810191845\n",
      "Epoch: 98 loss is: 0.2991141807268985 sino loss is: 0.9166228294372558 recon loss is: 0.20745189599503358\n",
      "Epoch: 99 loss is: 0.2884128145647947 sino loss is: 1.0665019035339356 recon loss is: 0.18176262450942435\n",
      "Epoch: 100 loss is: 0.27587059272566145 sino loss is: 0.9868432283401489 recon loss is: 0.17718626840153046\n",
      "Epoch: 101 loss is: 0.2639275870503955 sino loss is: 0.8417319059371948 recon loss is: 0.1797543949665599\n",
      "Epoch: 102 loss is: 0.2579590193460987 sino loss is: 0.8758374214172363 recon loss is: 0.1703752763103054\n",
      "Epoch: 103 loss is: 0.2503436699514672 sino loss is: 0.8715275049209594 recon loss is: 0.16319091767123187\n",
      "Epoch: 104 loss is: 0.24751993878223857 sino loss is: 0.8567955136299134 recon loss is: 0.16184038533308467\n",
      "Epoch: 105 loss is: 0.23734383500784484 sino loss is: 0.8072993516921997 recon loss is: 0.15661389745443907\n",
      "Epoch: 106 loss is: 0.2851039649359071 sino loss is: 1.3633625030517578 recon loss is: 0.1487677143327081\n",
      "Epoch: 107 loss is: 0.369289003938366 sino loss is: 1.8674295544624329 recon loss is: 0.18254604551189046\n",
      "Epoch: 108 loss is: 0.28232024852519166 sino loss is: 0.8885228037834167 recon loss is: 0.19346796814685\n",
      "Epoch: 109 loss is: 0.2645369917093485 sino loss is: 1.2019842386245727 recon loss is: 0.1443385645686358\n",
      "Epoch: 110 loss is: 0.258018683821049 sino loss is: 1.2366518378257751 recon loss is: 0.13435349705823926\n",
      "Epoch: 111 loss is: 0.22570997833928513 sino loss is: 0.772999107837677 recon loss is: 0.1484100660654013\n",
      "Epoch: 112 loss is: 0.21423160617533918 sino loss is: 0.7595108628273011 recon loss is: 0.1382805201906323\n",
      "Epoch: 113 loss is: 0.21284337193866648 sino loss is: 0.8675530791282654 recon loss is: 0.1260880634297935\n",
      "Epoch: 114 loss is: 0.19894095424744596 sino loss is: 0.7225004673004151 recon loss is: 0.12669090543124187\n",
      "Epoch: 115 loss is: 0.19325797827780547 sino loss is: 0.6668087363243103 recon loss is: 0.12657710211217704\n",
      "Epoch: 116 loss is: 0.19064750559327742 sino loss is: 0.698141086101532 recon loss is: 0.12083339579103135\n",
      "Epoch: 117 loss is: 0.19901227779315928 sino loss is: 0.8060560584068298 recon loss is: 0.11840667254852275\n",
      "Epoch: 118 loss is: 0.20111154500759282 sino loss is: 0.8192147850990296 recon loss is: 0.1191900644115273\n",
      "Epoch: 119 loss is: 0.1909289206533516 sino loss is: 0.7323540687561035 recon loss is: 0.11769351318169478\n",
      "Epoch: 120 loss is: 0.1845924288756154 sino loss is: 0.7087705254554748 recon loss is: 0.11371537454192858\n",
      "Epoch: 121 loss is: 0.18299895031181715 sino loss is: 0.7220795631408692 recon loss is: 0.11079099280563734\n",
      "Epoch: 122 loss is: 0.19869863353189213 sino loss is: 0.8777157545089722 recon loss is: 0.11092705688890203\n",
      "Epoch: 123 loss is: 0.2597594341692857 sino loss is: 1.4188354015350342 recon loss is: 0.11787589401578229\n",
      "Epoch: 124 loss is: 0.22828390447570582 sino loss is: 1.0439512014389039 recon loss is: 0.1238887838847806\n",
      "Epoch: 125 loss is: 0.21356684301301124 sino loss is: 1.029703140258789 recon loss is: 0.1105965274970162\n",
      "Epoch: 126 loss is: 0.19783238326040253 sino loss is: 0.936568534374237 recon loss is: 0.10417552743879305\n",
      "Epoch: 127 loss is: 0.18130162752220944 sino loss is: 0.7460211396217347 recon loss is: 0.10669951162288503\n",
      "Epoch: 128 loss is: 0.1697514145539276 sino loss is: 0.642001473903656 recon loss is: 0.10555126433234135\n",
      "Epoch: 129 loss is: 0.16905536862606454 sino loss is: 0.673499321937561 recon loss is: 0.10170543479318073\n",
      "Epoch: 130 loss is: 0.16407792590216355 sino loss is: 0.624889612197876 recon loss is: 0.1015889639373179\n",
      "Epoch: 131 loss is: 0.1629711396485904 sino loss is: 0.6203439831733704 recon loss is: 0.10093674058619528\n",
      "Epoch: 132 loss is: 0.15838047489373197 sino loss is: 0.5944611012935639 recon loss is: 0.09893436431734075\n",
      "Epoch: 133 loss is: 0.1610873431508661 sino loss is: 0.6304696798324585 recon loss is: 0.09804037442256217\n",
      "Epoch: 134 loss is: 0.1537902825071679 sino loss is: 0.5641308009624482 recon loss is: 0.09737720121883019\n",
      "Epoch: 135 loss is: 0.14979934922572008 sino loss is: 0.5412810444831848 recon loss is: 0.09567124403234355\n",
      "Epoch: 136 loss is: 0.15110071674490258 sino loss is: 0.5745063543319702 recon loss is: 0.09365007952356623\n",
      "Epoch: 137 loss is: 0.20847557296657765 sino loss is: 1.1298638105392456 recon loss is: 0.09548918863439763\n",
      "Epoch: 138 loss is: 0.16972508404690817 sino loss is: 0.6950289726257324 recon loss is: 0.10022218574125363\n",
      "Epoch: 139 loss is: 0.16202372888807232 sino loss is: 0.6949138283729553 recon loss is: 0.0925323447096723\n",
      "Epoch: 140 loss is: 0.2518979043269341 sino loss is: 1.6059724330902099 recon loss is: 0.09130065893175053\n",
      "Epoch: 141 loss is: 0.1976515133888016 sino loss is: 0.8997577786445617 recon loss is: 0.10767573165004352\n",
      "Epoch: 142 loss is: 0.1802575901542847 sino loss is: 0.8057007551193237 recon loss is: 0.09968751374828265\n",
      "Epoch: 143 loss is: 0.18177545824689123 sino loss is: 0.94264155626297 recon loss is: 0.087511299640362\n",
      "Epoch: 144 loss is: 0.154250371195511 sino loss is: 0.653084671497345 recon loss is: 0.08894190315170683\n",
      "Epoch: 145 loss is: 0.14311048500525475 sino loss is: 0.5286591947078705 recon loss is: 0.09024456478940963\n",
      "Epoch: 146 loss is: 0.14737478994231074 sino loss is: 0.6113641262054443 recon loss is: 0.0862383758316502\n",
      "Epoch: 147 loss is: 0.15523558066532084 sino loss is: 0.6862236738204956 recon loss is: 0.08661321134612034\n",
      "Epoch: 148 loss is: 0.1392362301293784 sino loss is: 0.5129034399986268 recon loss is: 0.08794588642753898\n",
      "Epoch: 149 loss is: 0.13673222896744003 sino loss is: 0.5183335959911346 recon loss is: 0.08489886907030335\n",
      "Epoch: 150 loss is: 0.13781060950357765 sino loss is: 0.5425817251205445 recon loss is: 0.08355243579943031\n",
      "Epoch: 151 loss is: 0.13993893634523694 sino loss is: 0.5480595588684082 recon loss is: 0.08513297956432644\n",
      "Epoch: 152 loss is: 0.14358355064297443 sino loss is: 0.5912615418434143 recon loss is: 0.08445739556456333\n",
      "Epoch: 153 loss is: 0.15952408193913387 sino loss is: 0.7540068984031677 recon loss is: 0.08412339120474743\n",
      "Epoch: 154 loss is: 0.14724088012206404 sino loss is: 0.6273242652416229 recon loss is: 0.08450845329987854\n",
      "Epoch: 155 loss is: 0.14159137218895831 sino loss is: 0.5800879120826721 recon loss is: 0.08358257919255177\n",
      "Epoch: 156 loss is: 0.13387318624598052 sino loss is: 0.5101712644100189 recon loss is: 0.08285605905992058\n",
      "Epoch: 157 loss is: 0.12910718399937451 sino loss is: 0.4658258676528931 recon loss is: 0.082524596936062\n",
      "Epoch: 158 loss is: 0.17372220632807028 sino loss is: 0.9038635611534118 recon loss is: 0.08333584842458974\n",
      "Epoch: 159 loss is: 0.17742816800344227 sino loss is: 0.9048448860645294 recon loss is: 0.0869436792479777\n",
      "Epoch: 160 loss is: 0.1573445115051862 sino loss is: 0.7041292130947113 recon loss is: 0.0869315903447267\n",
      "Epoch: 161 loss is: 0.15385337883263164 sino loss is: 0.7140326023101806 recon loss is: 0.08245011755853231\n",
      "Epoch: 162 loss is: 0.15122084771982666 sino loss is: 0.6874330759048461 recon loss is: 0.08247753908626075\n",
      "Epoch: 163 loss is: 0.166460663026203 sino loss is: 0.8140446543693542 recon loss is: 0.08505619609915147\n",
      "Epoch: 164 loss is: 0.14321592914504697 sino loss is: 0.585996824502945 recon loss is: 0.08461624624771766\n",
      "Epoch: 165 loss is: 0.14041208269813293 sino loss is: 0.5853120923042298 recon loss is: 0.08188087138154738\n",
      "Epoch: 166 loss is: 0.12980965920533466 sino loss is: 0.47916828989982607 recon loss is: 0.08189282887424755\n",
      "Epoch: 167 loss is: 0.12592385470905 sino loss is: 0.43769899010658264 recon loss is: 0.08215395495333368\n",
      "Epoch: 168 loss is: 0.12208521039731973 sino loss is: 0.4032495379447937 recon loss is: 0.08176025645382876\n",
      "Epoch: 169 loss is: 0.12206347305250273 sino loss is: 0.4056970000267029 recon loss is: 0.08149377275180922\n",
      "Epoch: 170 loss is: 0.12112325221154543 sino loss is: 0.3959125459194183 recon loss is: 0.08153199680003975\n",
      "Epoch: 171 loss is: 0.11845062526377761 sino loss is: 0.36853042244911194 recon loss is: 0.08159758227380835\n",
      "Epoch: 172 loss is: 0.12229533868181863 sino loss is: 0.4099407076835632 recon loss is: 0.08130126672136942\n",
      "Epoch: 173 loss is: 0.12343189928091401 sino loss is: 0.42209851145744326 recon loss is: 0.08122204716659422\n",
      "Epoch: 174 loss is: 0.1499153490107818 sino loss is: 0.6812526643276214 recon loss is: 0.08179008242900802\n",
      "Epoch: 175 loss is: 0.1391637584779429 sino loss is: 0.5632436215877533 recon loss is: 0.08283939646817919\n",
      "Epoch: 176 loss is: 0.1595010276521803 sino loss is: 0.7699752569198608 recon loss is: 0.08250350017205488\n",
      "Epoch: 177 loss is: 0.21883730502054732 sino loss is: 1.3265440821647645 recon loss is: 0.0861828944198851\n",
      "Epoch: 178 loss is: 0.15914624875096883 sino loss is: 0.6944294333457947 recon loss is: 0.08970330526737774\n",
      "Epoch: 179 loss is: 0.15075187901496026 sino loss is: 0.6846855282783508 recon loss is: 0.08228332469700905\n",
      "Epoch: 180 loss is: 0.14838578672804048 sino loss is: 0.6739293396472931 recon loss is: 0.08099285261429956\n",
      "Epoch: 181 loss is: 0.1332430589326744 sino loss is: 0.5015550553798676 recon loss is: 0.08308755190457154\n",
      "Epoch: 182 loss is: 0.1276685157428313 sino loss is: 0.4559605300426483 recon loss is: 0.08207246154647357\n",
      "Epoch: 183 loss is: 0.12324433106927896 sino loss is: 0.4277666449546814 recon loss is: 0.0804676652327063\n",
      "Epoch: 184 loss is: 0.11975989337942379 sino loss is: 0.3906989097595215 recon loss is: 0.08069000165841358\n",
      "Epoch: 185 loss is: 0.11672760804778798 sino loss is: 0.3571255624294281 recon loss is: 0.08101505038923486\n",
      "Epoch: 186 loss is: 0.11523077690411355 sino loss is: 0.34638814330101014 recon loss is: 0.0805919619034603\n",
      "Epoch: 187 loss is: 0.1148688656021747 sino loss is: 0.34227927923202517 recon loss is: 0.08064093708292575\n",
      "Epoch: 188 loss is: 0.11408728327707954 sino loss is: 0.33643061518669126 recon loss is: 0.08044422123686977\n",
      "Epoch: 189 loss is: 0.1153771614681959 sino loss is: 0.34859707951545715 recon loss is: 0.08051745314412116\n",
      "Epoch: 190 loss is: 0.11472433310283839 sino loss is: 0.34219516813755035 recon loss is: 0.08050481591655433\n",
      "Epoch: 191 loss is: 0.11541109527554523 sino loss is: 0.3479911744594574 recon loss is: 0.0806119780531169\n",
      "Epoch: 192 loss is: 0.12238809596490745 sino loss is: 0.41822679042816163 recon loss is: 0.08056541550648097\n",
      "Epoch: 193 loss is: 0.18730655918747835 sino loss is: 1.0430654644966126 recon loss is: 0.08300001109868937\n",
      "Epoch: 194 loss is: 0.1672133905041913 sino loss is: 0.7983363032341003 recon loss is: 0.08737975928671157\n",
      "Epoch: 195 loss is: 0.14303432363005897 sino loss is: 0.6059914529323578 recon loss is: 0.08243517759176514\n",
      "Epoch: 196 loss is: 0.14634327865629734 sino loss is: 0.6548975229263305 recon loss is: 0.08085352457552494\n",
      "Epoch: 197 loss is: 0.12638978482686813 sino loss is: 0.4507840692996979 recon loss is: 0.08131137685381706\n",
      "Epoch: 198 loss is: 0.13108998710397515 sino loss is: 0.49870250225067136 recon loss is: 0.08121973449472222\n",
      "Epoch: 199 loss is: 0.12686668750046204 sino loss is: 0.46455139517784116 recon loss is: 0.08041154783366633\n",
      "Epoch: 200 loss is: 0.12113500194530914 sino loss is: 0.4061445713043213 recon loss is: 0.0805205445168538\n",
      "Epoch: 201 loss is: 0.11685958876372626 sino loss is: 0.36480680108070374 recon loss is: 0.08037890791059783\n",
      "Epoch: 202 loss is: 0.11542982989238854 sino loss is: 0.3507347464561462 recon loss is: 0.08035635509776229\n",
      "Epoch: 203 loss is: 0.11581215174169118 sino loss is: 0.3547793447971344 recon loss is: 0.08033421696395453\n",
      "Epoch: 204 loss is: 0.11440308306767845 sino loss is: 0.34060351848602294 recon loss is: 0.0803427303995123\n",
      "Epoch: 205 loss is: 0.11531034585818875 sino loss is: 0.35031757950782777 recon loss is: 0.08027858723685372\n",
      "Epoch: 206 loss is: 0.11516600791746291 sino loss is: 0.3479574203491211 recon loss is: 0.08037026543551597\n",
      "Epoch: 207 loss is: 0.11462233433489497 sino loss is: 0.34243255853652954 recon loss is: 0.08037907736365493\n",
      "Epoch: 208 loss is: 0.11477626852897739 sino loss is: 0.34542017579078677 recon loss is: 0.08023425035385227\n",
      "Epoch: 209 loss is: 0.11582005999695268 sino loss is: 0.35254316926002505 recon loss is: 0.08056574172984567\n",
      "Epoch: 210 loss is: 0.11812854567523641 sino loss is: 0.37773447632789614 recon loss is: 0.08035509819145842\n",
      "Epoch: 211 loss is: 0.15398146510504707 sino loss is: 0.7314692854881286 recon loss is: 0.08083453551315292\n",
      "Epoch: 212 loss is: 0.1366172670778926 sino loss is: 0.5388382732868194 recon loss is: 0.08273343885514096\n",
      "Epoch: 213 loss is: 0.12760496865283874 sino loss is: 0.4641880691051483 recon loss is: 0.08118616099726586\n",
      "Epoch: 214 loss is: 0.17948280416342957 sino loss is: 0.9846722483634949 recon loss is: 0.08101557932708008\n",
      "Epoch: 215 loss is: 0.1283125319358186 sino loss is: 0.44876182079315186 recon loss is: 0.08343634911144536\n",
      "Epoch: 216 loss is: 0.1301874147110047 sino loss is: 0.4919420599937439 recon loss is: 0.08099320766854903\n",
      "Epoch: 217 loss is: 0.15275974482241525 sino loss is: 0.7219406843185425 recon loss is: 0.08056567594352616\n",
      "Epoch: 218 loss is: 0.1356395418323087 sino loss is: 0.5301175057888031 recon loss is: 0.08262779065738196\n",
      "Epoch: 219 loss is: 0.12729035353329293 sino loss is: 0.45970290899276733 recon loss is: 0.0813200626340162\n",
      "Epoch: 220 loss is: 0.1357031911424781 sino loss is: 0.5541801393032074 recon loss is: 0.08028517661611093\n",
      "Epoch: 221 loss is: 0.13111679368422768 sino loss is: 0.5019007563591004 recon loss is: 0.0809267174522712\n",
      "Epoch: 222 loss is: 0.12638295930088772 sino loss is: 0.44918608069419863 recon loss is: 0.08146434989036336\n",
      "Epoch: 223 loss is: 0.12310019702088712 sino loss is: 0.4283358931541443 recon loss is: 0.08026660725843786\n",
      "Epoch: 224 loss is: 0.11697555665231793 sino loss is: 0.36605721712112427 recon loss is: 0.08036983419514745\n",
      "Epoch: 225 loss is: 0.11439546564837862 sino loss is: 0.3391733288764954 recon loss is: 0.08047813209017683\n",
      "Epoch: 226 loss is: 0.11411075601956783 sino loss is: 0.33764591813087463 recon loss is: 0.08034616383395134\n",
      "Epoch: 227 loss is: 0.11350294740220308 sino loss is: 0.33180380463600156 recon loss is: 0.08032256604453325\n",
      "Epoch: 228 loss is: 0.11347603578399289 sino loss is: 0.3312718629837036 recon loss is: 0.08034884896408188\n",
      "Epoch: 229 loss is: 0.11299751071440937 sino loss is: 0.32652627825737 recon loss is: 0.08034488266515495\n",
      "Epoch: 230 loss is: 0.11944062750542189 sino loss is: 0.3906908452510834 recon loss is: 0.08037154119217421\n",
      "Epoch: 231 loss is: 0.11409043085903202 sino loss is: 0.33624024987220763 recon loss is: 0.08046640467971836\n",
      "Epoch: 232 loss is: 0.11476477047499059 sino loss is: 0.3436407148838043 recon loss is: 0.08040069749649405\n",
      "Epoch: 233 loss is: 0.1261480530285199 sino loss is: 0.4575116991996765 recon loss is: 0.08039688295954063\n",
      "Epoch: 234 loss is: 0.11925372479205057 sino loss is: 0.38446907997131347 recon loss is: 0.08080681672041343\n",
      "Epoch: 235 loss is: 0.11833890962597823 sino loss is: 0.3795070230960846 recon loss is: 0.08038820612427687\n",
      "Epoch: 236 loss is: 0.143750882822206 sino loss is: 0.6300818800926209 recon loss is: 0.08074269362085103\n",
      "Epoch: 237 loss is: 0.1311332767118622 sino loss is: 0.4951203942298889 recon loss is: 0.08162123609678042\n",
      "Epoch: 238 loss is: 0.15338452890037033 sino loss is: 0.7211363196372986 recon loss is: 0.08127089648960563\n",
      "Epoch: 239 loss is: 0.16442268728270643 sino loss is: 0.8249884963035583 recon loss is: 0.08192383586421123\n",
      "Epoch: 240 loss is: 0.13246082349806945 sino loss is: 0.5033366918563843 recon loss is: 0.08212715371638458\n",
      "Epoch: 241 loss is: 0.12742707614945245 sino loss is: 0.4702302932739258 recon loss is: 0.08040404652403665\n",
      "Epoch: 242 loss is: 0.12290257386882661 sino loss is: 0.42452319860458376 recon loss is: 0.08045025341232179\n",
      "Epoch: 243 loss is: 0.11764994381313029 sino loss is: 0.37149789929389954 recon loss is: 0.0805001535112113\n",
      "Epoch: 244 loss is: 0.11786110833738027 sino loss is: 0.37625877261161805 recon loss is: 0.08023523115072427\n",
      "Epoch: 245 loss is: 0.11457577406242489 sino loss is: 0.3432361543178558 recon loss is: 0.0802521584816277\n",
      "Epoch: 246 loss is: 0.11559211491827041 sino loss is: 0.35325907468795775 recon loss is: 0.08026620662991077\n",
      "Epoch: 247 loss is: 0.11852787749705117 sino loss is: 0.37941298484802244 recon loss is: 0.08058657886323732\n",
      "Epoch: 248 loss is: 0.1233922110229961 sino loss is: 0.42848119139671326 recon loss is: 0.08054409113826672\n",
      "Epoch: 249 loss is: 0.12721109098799407 sino loss is: 0.46707690954208375 recon loss is: 0.08050339973576247\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m loss_recon_domain \u001b[39m=\u001b[39m mse_fn(phantom_batch \u001b[39m-\u001b[39m recons)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_recon_domain \u001b[39m+\u001b[39m loss_sino_domain\u001b[39m*\u001b[39m\u001b[39m0.1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m batch_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from utils.data import get_htc2022_train_phantoms, get_kits_train_phantoms, get_htclike_train_phantoms\n",
    "from utils.polynomials import Legendre, Chebyshev\n",
    "from geometries import FlatFanBeamGeometry, DEVICE, HTC2022_GEOMETRY, ParallelGeometry\n",
    "from geometries.geometry_base import naive_sino_filling, mark_cyclic\n",
    "from models.fbps import AdaptiveFBP as AFBP\n",
    "from models.FNOBPs.fnobp import FNO_BP\n",
    "from models.SerieBPs.series_bp1 import Series_BP\n",
    "from models.modelbase import plot_model_progress\n",
    "from statistics import mean\n",
    "\n",
    "ar = 0.25 #angle ratio of full 360 deg scan\n",
    "PHANTOM_DATA = torch.concat([get_htc2022_train_phantoms(), get_htclike_train_phantoms()])\n",
    "geometry = HTC2022_GEOMETRY\n",
    "# PHANTOM_DATA = get_kits_train_phantoms()\n",
    "# geometry = FlatFanBeamGeometry(720, 560, 410.66, 543.74, 112, [-40,40, -40, 40], [256, 256])\n",
    "# geometry = FlatFanBeamGeometry(1800, 300, 1.5, 3.0, 4.0, [-1,1,-1,1], [256, 256])\n",
    "# geometry = ParallelGeometry(1800, 300, [-1,1,-1,1], [256, 256])\n",
    "N_known_angles = int(geometry.n_projections*ar)\n",
    "N_angles_out = int(geometry.n_projections*0.6) #can be 0.5 if parallel beam\n",
    "SINO_DATA = geometry.project_forward(PHANTOM_DATA)\n",
    "print(SINO_DATA.dtype, SINO_DATA.device, SINO_DATA.shape)\n",
    "\n",
    "# model = AFBP(geometry)\n",
    "# model = FNO_BP(geometry, hidden_layers=[40,40], n_known_angles=N_known_angles, n_angles_out=N_angles_out)\n",
    "model = Series_BP(geometry, ar, 120, 60, Legendre.key)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(SINO_DATA, PHANTOM_DATA)\n",
    "dataloader = DataLoader(dataset, batch_size=6, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "mse_fn = lambda diff : torch.mean(diff**2)\n",
    "n_epochs = 800\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses, batch_sino_losses, batch_recon_losses = [], [], []\n",
    "    for sino_batch, phantom_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_ind = 0 #torch.randint(0, geometry.n_projections, (1,)).item()\n",
    "        known_beta_bool = torch.zeros(geometry.n_projections, device=DEVICE, dtype=bool)\n",
    "        out_beta_bool = known_beta_bool.clone()\n",
    "        mark_cyclic(known_beta_bool, start_ind, (start_ind+N_known_angles)%geometry.n_projections)#known_beta_bool is True at angles where sinogram is meassured and false otherwise\n",
    "        mark_cyclic(out_beta_bool, start_ind, (start_ind+N_angles_out)%geometry.n_projections)\n",
    "        la_sinos = sino_batch * 0 #limited angle sinograms\n",
    "        la_sinos[:, known_beta_bool] = sino_batch[:, known_beta_bool]\n",
    "\n",
    "        filtered = model.get_extrapolated_filtered_sinos(la_sinos, known_beta_bool, out_beta_bool)\n",
    "        gt_filtered = geometry.inverse_fourier_transform(geometry.fourier_transform(sino_batch*geometry.jacobian_det)*geometry.ram_lak_filter())\n",
    "        loss_sino_domain = mse_fn(gt_filtered-filtered)\n",
    "\n",
    "        recons = F.relu(geometry.project_backward(filtered/2)) #sinogram covers 360deg  - double coverage\n",
    "        loss_recon_domain = mse_fn(phantom_batch - recons)\n",
    "\n",
    "        loss = loss_recon_domain + loss_sino_domain*0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().item())\n",
    "        batch_sino_losses.append(loss_sino_domain.cpu().item())\n",
    "        batch_recon_losses.append(loss_recon_domain.cpu().item())\n",
    "    \n",
    "    print(\"Epoch:\", epoch+1, \"loss is:\", mean(batch_losses), \"sino loss is:\", mean(batch_sino_losses), \"recon loss is:\", mean(batch_recon_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfbps\u001b[39;00m \u001b[39mimport\u001b[39;00m FBP\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m fbp \u001b[39m=\u001b[39m FBP(model\u001b[39m.\u001b[39;49mgeometry)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#Clear previous plots\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m plt\u001b[39m.\u001b[39mget_fignums():\n",
      "File \u001b[0;32m~/deep-limited-angle/KEX---CT-reconstruction/src/models/fbps.py:51\u001b[0m, in \u001b[0;36mFBP.__init__\u001b[0;34m(self, geometry, kernel)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeometry \u001b[39m=\u001b[39m geometry\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m kernel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     kernel \u001b[39m=\u001b[39m geometry\u001b[39m.\u001b[39;49mram_lak_filter()\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(kernel\u001b[39m.\u001b[39mto(DEVICE, dtype\u001b[39m=\u001b[39mCDTYPE), requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/deep-limited-angle/KEX---CT-reconstruction/src/geometries/fanbeam_geometry/fanbeam_geometry.py:167\u001b[0m, in \u001b[0;36mFlatFanBeamGeometry.ram_lak_filter\u001b[0;34m(self, cutoff_ratio, full_size)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mram_lak_filter\u001b[39m(\u001b[39mself\u001b[39m, cutoff_ratio: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, full_size \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 167\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mws\u001b[39m.\u001b[39;49mto(CDTYPE) \u001b[39m/\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39mpi)\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m cutoff_ratio \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m         k[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mws \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mws\u001b[39m.\u001b[39mmax()\u001b[39m*\u001b[39mcutoff_ratio] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from models.fbps import FBP\n",
    "fbp = FBP(model.geometry)\n",
    "#Clear previous plots\n",
    "for i in plt.get_fignums():\n",
    "    plt.figure(i)\n",
    "    plt.close()\n",
    "\n",
    "zero_cropped_sinos, known_region = geometry.reflect_fill_sinos(*geometry.zero_cropp_sinos(SINO_DATA[:5], ar, 0))\n",
    "angles = torch.zeros(geometry.n_projections, dtype=bool, device=DEVICE)\n",
    "known_angles = mark_cyclic(angles.clone(), 0, N_known_angles)\n",
    "out_angles = mark_cyclic(angles.clone(), 0, N_angles_out)\n",
    "# zero_cropped_sinos = naive_sino_filling(zero_cropped_sinos, (~known_region).sum(dim=-1) == 0)\n",
    "disp_ind = 2\n",
    "plot_model_progress(model, SINO_DATA[:5], known_angles, out_angles, PHANTOM_DATA[:5], disp_ind=disp_ind)\n",
    "plot_model_progress(fbp, SINO_DATA[:5], known_angles, out_angles, PHANTOM_DATA[:5], disp_ind=disp_ind)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full angle fbp recons mse\n",
    "recons = geometry.fbp_reconstruct(SINO_DATA)\n",
    "print(torch.mean((recons-PHANTOM_DATA)**2).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to /home/emastr/deep-limited-angle/KEX---CT-reconstruction/data/models/fno_bp_fanbeamkits_ar0.25.pt\n"
     ]
    }
   ],
   "source": [
    "#Save model\n",
    "modelname = \"fno_bp_fanbeamkits_ar0.25\"\n",
    "from models.modelbase import save_model_checkpoint\n",
    "save_path = GIT_ROOT / \"data\" / \"models\" / (modelname + \".pt\")\n",
    "save_model_checkpoint(model, optimizer, loss, ar, save_path)\n",
    "print(\"model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odl_torch",
   "language": "python",
   "name": "odl_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
