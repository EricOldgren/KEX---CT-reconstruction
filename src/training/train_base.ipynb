{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "matplotlib.use(\"WebAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "GIT_ROOT = Path(\"../..\").resolve()\n",
    "SRC = GIT_ROOT / \"src\"\n",
    "if not SRC in sys.path:\n",
    "    sys.path.append(str(SRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512, 512])\n",
      "torch.Size([2, 512, 512])\n",
      "torch.Size([3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "from utils.data import get_htc2022_train_phantoms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "HTC_PHANTOMS = get_htc2022_train_phantoms()\n",
    "print(HTC_PHANTOMS.shape)\n",
    "HTC_TRAIN_PHANTOMS, VALIDATION_PHANTOMS = train_test_split(HTC_PHANTOMS, test_size=3)\n",
    "\n",
    "print(HTC_TRAIN_PHANTOMS.shape)\n",
    "print(VALIDATION_PHANTOMS.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 cuda:0 torch.Size([86, 720, 560])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emastr/anaconda3/envs/odl_torch/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755849709/work/aten/src/ATen/native/Copy.cpp:239.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss is: 0.5024940176691732 sino loss is: 0.6405034254897725 recon loss is: 0.4384436744428704\n",
      "Epoch: 2 loss is: 0.48824728201865575 sino loss is: 0.455335332588716 recon loss is: 0.44271374821792375\n",
      "Epoch: 3 loss is: 0.48581546415689886 sino loss is: 0.4451517273079265 recon loss is: 0.44130029027465284\n",
      "Epoch: 4 loss is: 0.4852509280078704 sino loss is: 0.4382050335407257 recon loss is: 0.4414304239087397\n",
      "Epoch: 5 loss is: 0.48470636900000036 sino loss is: 0.43378400802612305 recon loss is: 0.44132796779099276\n",
      "Epoch: 6 loss is: 0.48504277311333227 sino loss is: 0.428209207274697 recon loss is: 0.44222185211493237\n",
      "Epoch: 7 loss is: 0.483845809793083 sino loss is: 0.42493477734652435 recon loss is: 0.44135233165203525\n",
      "Epoch: 8 loss is: 0.4831060509856447 sino loss is: 0.42217493057250977 recon loss is: 0.44088855758973095\n",
      "Epoch: 9 loss is: 0.4823512588544299 sino loss is: 0.4192949641834606 recon loss is: 0.4404217615555607\n",
      "Epoch: 10 loss is: 0.48216581865824876 sino loss is: 0.41703456369313324 recon loss is: 0.4404623614761448\n",
      "Epoch: 11 loss is: 0.48169458440236573 sino loss is: 0.4154565794901414 recon loss is: 0.44014892509870057\n",
      "Epoch: 12 loss is: 0.48090038023376586 sino loss is: 0.413962494243275 recon loss is: 0.43950412945478734\n",
      "Epoch: 13 loss is: 0.4801556288130305 sino loss is: 0.4132022505456751 recon loss is: 0.43883540274247473\n",
      "Epoch: 14 loss is: 0.47978290877031304 sino loss is: 0.4120318727059798 recon loss is: 0.4385797211610523\n",
      "Epoch: 15 loss is: 0.47952278891265615 sino loss is: 0.41270290179686114 recon loss is: 0.4382524983265747\n",
      "Epoch: 16 loss is: 0.4791466640529277 sino loss is: 0.41140354492447595 recon loss is: 0.4380063088831546\n",
      "Epoch: 17 loss is: 0.4797346468855182 sino loss is: 0.41350559212944726 recon loss is: 0.4383840871307131\n",
      "Epoch: 18 loss is: 0.47946349649121645 sino loss is: 0.4139479263262315 recon loss is: 0.4380687029780701\n",
      "Epoch: 19 loss is: 0.47813262982125637 sino loss is: 0.415731432763013 recon loss is: 0.43655948600309463\n",
      "Epoch: 20 loss is: 0.47754939075007924 sino loss is: 0.4192080335183577 recon loss is: 0.43562858658545284\n",
      "Epoch: 21 loss is: 0.4768459263150707 sino loss is: 0.4233784160830758 recon loss is: 0.43450808402943764\n",
      "Epoch: 22 loss is: 0.47589139283708504 sino loss is: 0.42958877845243976 recon loss is: 0.43293251485637596\n",
      "Epoch: 23 loss is: 0.4752053308484116 sino loss is: 0.4343879602172158 recon loss is: 0.43176653387843433\n",
      "Epoch: 24 loss is: 0.4742678887497739 sino loss is: 0.44261630285869946 recon loss is: 0.4300062577865784\n",
      "Epoch: 25 loss is: 0.47322087050557643 sino loss is: 0.4523316567594355 recon loss is: 0.4279877034749819\n",
      "Epoch: 26 loss is: 0.4708183018711111 sino loss is: 0.4649241024797613 recon loss is: 0.42432589121673964\n",
      "Epoch: 27 loss is: 0.46902132872558777 sino loss is: 0.47980344837362116 recon loss is: 0.42104098260130723\n",
      "Epoch: 28 loss is: 0.46742864680518215 sino loss is: 0.4923816648396579 recon loss is: 0.418190479779356\n",
      "Epoch: 29 loss is: 0.46422936635219497 sino loss is: 0.5132201530716636 recon loss is: 0.4129073505031682\n",
      "Epoch: 30 loss is: 0.4626788644518489 sino loss is: 0.5330039100213484 recon loss is: 0.40937847223052815\n",
      "Epoch: 31 loss is: 0.4602942943181077 sino loss is: 0.5504317283630371 recon loss is: 0.4052511201948855\n",
      "Epoch: 32 loss is: 0.45715072648458605 sino loss is: 0.5686364661563527 recon loss is: 0.400287079530288\n",
      "Epoch: 33 loss is: 0.4543154553967111 sino loss is: 0.5896425301378424 recon loss is: 0.3953512012314735\n",
      "Epoch: 34 loss is: 0.4518541577045545 sino loss is: 0.6064593412659385 recon loss is: 0.3912082228329026\n",
      "Epoch: 35 loss is: 0.4492719677156438 sino loss is: 0.6246351491321217 recon loss is: 0.38680845049952495\n",
      "Epoch: 36 loss is: 0.44700129999054056 sino loss is: 0.6396720138463107 recon loss is: 0.3830340974544561\n",
      "Epoch: 37 loss is: 0.44344469634387484 sino loss is: 0.6576971520077098 recon loss is: 0.3776749799239179\n",
      "Epoch: 38 loss is: 0.43987286203863823 sino loss is: 0.6792389425364408 recon loss is: 0.37194896602394784\n",
      "Epoch: 39 loss is: 0.43655590944941974 sino loss is: 0.7030551867051558 recon loss is: 0.3662503899661136\n",
      "Epoch: 40 loss is: 0.43135146175117967 sino loss is: 0.7246706756678495 recon loss is: 0.3588843920169531\n",
      "Epoch: 41 loss is: 0.42592348515762735 sino loss is: 0.7653511600060896 recon loss is: 0.3493883679378325\n",
      "Epoch: 42 loss is: 0.41789345481520085 sino loss is: 0.8180010318756104 recon loss is: 0.3360933495956633\n",
      "Epoch: 43 loss is: 0.40518073599916177 sino loss is: 0.8776477575302124 recon loss is: 0.3174159580786989\n",
      "Epoch: 44 loss is: 0.40017474217727234 sino loss is: 0.9348228898915377 recon loss is: 0.3066924510206769\n",
      "Epoch: 45 loss is: 0.3784131767805936 sino loss is: 1.0349127758633008 recon loss is: 0.27492189892333335\n",
      "Epoch: 46 loss is: 0.3518046360101996 sino loss is: 1.0158631530675022 recon loss is: 0.2502183185360078\n",
      "Epoch: 47 loss is: 0.32511948781142364 sino loss is: 0.9680736606771295 recon loss is: 0.22831212038905968\n",
      "Epoch: 48 loss is: 0.32861652113595585 sino loss is: 1.0362668362530796 recon loss is: 0.2249898357496016\n",
      "Epoch: 49 loss is: 0.2965102073266481 sino loss is: 0.9661442312327299 recon loss is: 0.19989578284872414\n",
      "Epoch: 50 loss is: 0.2716576030642629 sino loss is: 0.8882450732317838 recon loss is: 0.18283309519922414\n",
      "Epoch: 51 loss is: 0.2595218235849084 sino loss is: 0.8674716949462891 recon loss is: 0.17277465205830297\n",
      "Epoch: 52 loss is: 0.2536789517542693 sino loss is: 0.8813511349938132 recon loss is: 0.16554383798395775\n",
      "Epoch: 53 loss is: 0.24681173515006838 sino loss is: 0.8701415874741294 recon loss is: 0.15979757376108594\n",
      "Epoch: 54 loss is: 0.25984792970676845 sino loss is: 1.0563689795407383 recon loss is: 0.154211030939904\n",
      "Epoch: 55 loss is: 0.24198527102947714 sino loss is: 1.0396346883340315 recon loss is: 0.13802179975770215\n",
      "Epoch: 56 loss is: 0.20795757545166987 sino loss is: 0.6854408058253202 recon loss is: 0.13941349446274254\n",
      "Epoch: 57 loss is: 0.18444576246586863 sino loss is: 0.6194476729089563 recon loss is: 0.12250099429444986\n",
      "Epoch: 58 loss is: 0.18092367151295496 sino loss is: 0.6587088649923151 recon loss is: 0.11505278365907244\n",
      "Epoch: 59 loss is: 0.19219196112561335 sino loss is: 0.7375279746272347 recon loss is: 0.11843916264690162\n",
      "Epoch: 60 loss is: 0.1865604309874607 sino loss is: 0.7522382600740953 recon loss is: 0.1113366034222025\n",
      "Epoch: 61 loss is: 0.16301137641805388 sino loss is: 0.5355991748246279 recon loss is: 0.10945145778413772\n",
      "Epoch: 62 loss is: 0.15400443953419976 sino loss is: 0.4607402357188138 recon loss is: 0.10793041453993481\n",
      "Epoch: 63 loss is: 0.14864378064432424 sino loss is: 0.4043653905391693 recon loss is: 0.1082072405066865\n",
      "Epoch: 64 loss is: 0.1445301867505712 sino loss is: 0.3659332367506894 recon loss is: 0.10793686199178143\n",
      "Epoch: 65 loss is: 0.14361855385259314 sino loss is: 0.362172394990921 recon loss is: 0.10740131374390809\n",
      "Epoch: 66 loss is: 0.14532668250423567 sino loss is: 0.3832882426001809 recon loss is: 0.10699785777008973\n",
      "Epoch: 67 loss is: 0.14308537164645135 sino loss is: 0.3588798858902671 recon loss is: 0.10719738285422699\n",
      "Epoch: 68 loss is: 0.14269078277280028 sino loss is: 0.35620625994422217 recon loss is: 0.10707015569465726\n",
      "Epoch: 69 loss is: 0.15007517284153357 sino loss is: 0.43115066669204016 recon loss is: 0.10696010563046915\n",
      "Epoch: 70 loss is: 0.1718326476365601 sino loss is: 0.6303318359635093 recon loss is: 0.10879946207596518\n",
      "Epoch: 71 loss is: 0.2029334439172138 sino loss is: 0.937525749206543 recon loss is: 0.1091808687256293\n",
      "Epoch: 72 loss is: 0.1969149495513665 sino loss is: 0.7321107550100847 recon loss is: 0.12370387228931173\n",
      "Epoch: 73 loss is: 0.16846856710163635 sino loss is: 0.6137170710346915 recon loss is: 0.10709685830485342\n",
      "Epoch: 74 loss is: 0.14900397981278915 sino loss is: 0.40979068116708234 recon loss is: 0.10802491108648796\n",
      "Epoch: 75 loss is: 0.14379726850488428 sino loss is: 0.3726788434115323 recon loss is: 0.10652938308001024\n",
      "Epoch: 76 loss is: 0.14040598403958787 sino loss is: 0.3418259999968789 recon loss is: 0.10622338285458034\n",
      "Epoch: 77 loss is: 0.13880113417933734 sino loss is: 0.32858514243906195 recon loss is: 0.10594261952903586\n",
      "Epoch: 78 loss is: 0.13851069482327572 sino loss is: 0.3212952207435261 recon loss is: 0.10638117257959172\n",
      "Epoch: 79 loss is: 0.1364447642240184 sino loss is: 0.3120272999460047 recon loss is: 0.10524203358595871\n",
      "Epoch: 80 loss is: 0.13657768649236696 sino loss is: 0.31321698698130523 recon loss is: 0.10525598701531208\n",
      "Epoch: 81 loss is: 0.13711502465806874 sino loss is: 0.3110690333626487 recon loss is: 0.10600812050901325\n",
      "Epoch: 82 loss is: 0.13561044224559704 sino loss is: 0.30704857544465497 recon loss is: 0.10490558412540488\n",
      "Epoch: 83 loss is: 0.13943362486950261 sino loss is: 0.3464306511662223 recon loss is: 0.10479055900782233\n",
      "Epoch: 84 loss is: 0.1399015180207025 sino loss is: 0.3474909988316623 recon loss is: 0.10515241752794331\n",
      "Epoch: 85 loss is: 0.14456916747122867 sino loss is: 0.39372155612165277 recon loss is: 0.1051970110124065\n",
      "Epoch: 86 loss is: 0.19246293865846917 sino loss is: 0.8676127628846602 recon loss is: 0.10570166115081724\n",
      "Epoch: 87 loss is: 0.20080187652127665 sino loss is: 0.7550533468073065 recon loss is: 0.1252965412986856\n",
      "Epoch: 88 loss is: 0.17860441139749467 sino loss is: 0.7317596999081698 recon loss is: 0.10542844025522433\n",
      "Epoch: 89 loss is: 0.14666610208405373 sino loss is: 0.40339546311985364 recon loss is: 0.10632655529794051\n",
      "Epoch: 90 loss is: 0.14085797844848724 sino loss is: 0.3554544990712946 recon loss is: 0.10531252806722992\n",
      "Epoch: 91 loss is: 0.13569827043077612 sino loss is: 0.31364124742421234 recon loss is: 0.10433414531582584\n",
      "Epoch: 92 loss is: 0.13450503081998313 sino loss is: 0.3030000329017639 recon loss is: 0.10420502742820792\n",
      "Epoch: 93 loss is: 0.13385373701795608 sino loss is: 0.2962077422575517 recon loss is: 0.1042329622503405\n",
      "Epoch: 94 loss is: 0.133677496864768 sino loss is: 0.29386248100887646 recon loss is: 0.10429124825588622\n",
      "Epoch: 95 loss is: 0.1351954710858158 sino loss is: 0.2977867668325251 recon loss is: 0.10541679379297035\n",
      "Epoch: 96 loss is: 0.1341940199978299 sino loss is: 0.29720522598786786 recon loss is: 0.10447349685718271\n",
      "Epoch: 97 loss is: 0.1422279231885296 sino loss is: 0.37734918973662634 recon loss is: 0.10449300319887868\n",
      "Epoch: 98 loss is: 0.147394227868042 sino loss is: 0.4256659150123596 recon loss is: 0.10482763514762013\n",
      "Epoch: 99 loss is: 0.14700116875394864 sino loss is: 0.42426724054596643 recon loss is: 0.10457444392042765\n",
      "Epoch: 100 loss is: 0.16538715062707451 sino loss is: 0.5979398326440291 recon loss is: 0.10559316594028804\n",
      "Epoch: 101 loss is: 0.14871657066043822 sino loss is: 0.4363911883397536 recon loss is: 0.10507745128460248\n",
      "Epoch: 102 loss is: 0.14193399089249698 sino loss is: 0.37808622555299237 recon loss is: 0.10412536772760479\n",
      "Epoch: 103 loss is: 0.13560118067544222 sino loss is: 0.3187395266511224 recon loss is: 0.10372722740073702\n",
      "Epoch: 104 loss is: 0.13688045470563248 sino loss is: 0.3272112878886136 recon loss is: 0.10415932537491072\n",
      "Epoch: 105 loss is: 0.14020484026293387 sino loss is: 0.36030624129555444 recon loss is: 0.10417421501579133\n",
      "Epoch: 106 loss is: 0.1416764647324399 sino loss is: 0.37887257608500396 recon loss is: 0.10378920610795124\n",
      "Epoch: 107 loss is: 0.13631331400771574 sino loss is: 0.32497330958193 recon loss is: 0.10381598233833093\n",
      "Epoch: 108 loss is: 0.14028941851730128 sino loss is: 0.35952557216991077 recon loss is: 0.10433686113097884\n",
      "Epoch: 109 loss is: 0.1377860974294909 sino loss is: 0.3357820456678217 recon loss is: 0.10420789174512166\n",
      "Epoch: 110 loss is: 0.15164438786782497 sino loss is: 0.4673528481613506 recon loss is: 0.10490910250982952\n",
      "Epoch: 111 loss is: 0.1550882511922221 sino loss is: 0.5047490515492179 recon loss is: 0.10461334535997481\n",
      "Epoch: 112 loss is: 0.14698442399438377 sino loss is: 0.42181247201832855 recon loss is: 0.10480317614909168\n",
      "Epoch: 113 loss is: 0.13854254890771783 sino loss is: 0.33955950899557635 recon loss is: 0.10458659726310214\n",
      "Epoch: 114 loss is: 0.1380981409421096 sino loss is: 0.33657170967622235 recon loss is: 0.10444096892463281\n",
      "Epoch: 115 loss is: 0.14300056492745805 sino loss is: 0.39006825197826733 recon loss is: 0.10399373871364306\n",
      "Epoch: 116 loss is: 0.1698252588258759 sino loss is: 0.6222516081549905 recon loss is: 0.10760009635092936\n",
      "Epoch: 117 loss is: 0.15777119936973877 sino loss is: 0.534397388046438 recon loss is: 0.10433145910884513\n",
      "Epoch: 118 loss is: 0.14099742688216924 sino loss is: 0.36111939495260065 recon loss is: 0.10488548684504877\n",
      "Epoch: 119 loss is: 0.13558445144568332 sino loss is: 0.31239082596518775 recon loss is: 0.10434536823957156\n",
      "Epoch: 120 loss is: 0.14051271726227507 sino loss is: 0.3609392453323711 recon loss is: 0.10441879205171246\n",
      "Epoch: 121 loss is: 0.13673219664746566 sino loss is: 0.3218166340481151 recon loss is: 0.10455053253146238\n",
      "Epoch: 122 loss is: 0.13740480770168434 sino loss is: 0.33016917922280054 recon loss is: 0.10438788964393918\n",
      "Epoch: 123 loss is: 0.13891869127528195 sino loss is: 0.3488578796386719 recon loss is: 0.1040329024308916\n",
      "Epoch: 124 loss is: 0.15596336125334384 sino loss is: 0.4998316331343217 recon loss is: 0.10598019695778968\n",
      "Epoch: 125 loss is: 0.16029217369197904 sino loss is: 0.5564318543130701 recon loss is: 0.10464898738014887\n",
      "Epoch: 126 loss is: 0.15130587374323534 sino loss is: 0.46039348569783295 recon loss is: 0.10526652408973122\n",
      "Epoch: 127 loss is: 0.14220687956169611 sino loss is: 0.37750772454521875 recon loss is: 0.10445610676851148\n",
      "Epoch: 128 loss is: 0.13800585246684838 sino loss is: 0.3418069210919467 recon loss is: 0.10382515998512468\n",
      "Epoch: 129 loss is: 0.13737935164532233 sino loss is: 0.33223051916469226 recon loss is: 0.10415629955952171\n",
      "Epoch: 130 loss is: 0.13449361583637096 sino loss is: 0.3023373945192857 recon loss is: 0.104259876215111\n",
      "Epoch: 131 loss is: 0.13428067891001577 sino loss is: 0.30274705724282697 recon loss is: 0.10400597261000638\n",
      "Epoch: 132 loss is: 0.1374758049151588 sino loss is: 0.3318142525174401 recon loss is: 0.10429437871515908\n",
      "Epoch: 133 loss is: 0.1596880612160516 sino loss is: 0.5462680729952726 recon loss is: 0.10506125279893727\n",
      "Epoch: 134 loss is: 0.15001478526974066 sino loss is: 0.449078307910399 recon loss is: 0.10510695325951484\n",
      "Epoch: 135 loss is: 0.14350869351603246 sino loss is: 0.3934256434440613 recon loss is: 0.10416612856203337\n",
      "Epoch: 136 loss is: 0.136835213345751 sino loss is: 0.32886213064193726 recon loss is: 0.10394899997676078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m gt_filtered \u001b[39m=\u001b[39m geometry\u001b[39m.\u001b[39minverse_fourier_transform(geometry\u001b[39m.\u001b[39mfourier_transform(sino_batch\u001b[39m*\u001b[39mgeometry\u001b[39m.\u001b[39mjacobian_det)\u001b[39m*\u001b[39mgeometry\u001b[39m.\u001b[39mram_lak_filter())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m loss_sino_domain \u001b[39m=\u001b[39m mse_fn(gt_filtered\u001b[39m-\u001b[39mfiltered)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m recons \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(geometry\u001b[39m.\u001b[39;49mproject_backward(filtered\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m)) \u001b[39m#sinogram covers 360deg  - double coverage\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m loss_recon_domain \u001b[39m=\u001b[39m mse_fn(phantom_batch \u001b[39m-\u001b[39m recons)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshepp.math.kth.se/home/emastr/deep-limited-angle/KEX---CT-reconstruction/src/training/train_base.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_recon_domain \u001b[39m+\u001b[39m loss_sino_domain\u001b[39m*\u001b[39m\u001b[39m0.1\u001b[39m\n",
      "File \u001b[0;32m~/deep-limited-angle/KEX---CT-reconstruction/src/geometries/fanbeam_geometry/fanbeam_geometry.py:164\u001b[0m, in \u001b[0;36mFlatFanBeamGeometry.project_backward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mproject_backward\u001b[39m(\u001b[39mself\u001b[39m, X: torch\u001b[39m.\u001b[39mTensor)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39mtorch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    163\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWegthed BP operator to use for FBP algorithm\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBP(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/odl/contrib/torch/operator.py:496\u001b[0m, in \u001b[0;36mOperatorModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    491\u001b[0m     shp_str \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(op_in_shape)\u001b[39m.\u001b[39mstrip(\u001b[39m'\u001b[39m\u001b[39m()\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    492\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    493\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput tensor has wrong shape: expected (N, *, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m), got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    494\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(shp_str, in_shape)\n\u001b[1;32m    495\u001b[0m     )\n\u001b[0;32m--> 496\u001b[0m \u001b[39mreturn\u001b[39;00m OperatorFunction\u001b[39m.\u001b[39;49mapply(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moperator, x)\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/odl/contrib/torch/operator.py:239\u001b[0m, in \u001b[0;36mOperatorFunction.forward\u001b[0;34m(ctx, operator, input)\u001b[0m\n\u001b[1;32m    237\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    238\u001b[0m \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m input_arr_flat_extra:\n\u001b[0;32m--> 239\u001b[0m     results\u001b[39m.\u001b[39mappend(operator(inp))\n\u001b[1;32m    241\u001b[0m \u001b[39m# Stack results, reshape to the expected output shape and enforce\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39m# correct dtype\u001b[39;00m\n\u001b[1;32m    243\u001b[0m result_arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(results)\u001b[39m.\u001b[39mastype(op_out_dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/odl/operator/operator.py:695\u001b[0m, in \u001b[0;36mOperator.__call__\u001b[0;34m(self, x, out, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`op` returned a different value than `out`. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    690\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mWith in-place evaluation, the operator can \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    691\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39monly return nothing (`None`) or the `out` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    692\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mparameter.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    694\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Out-of-place evaluation\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_out_of_place(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    697\u001b[0m     \u001b[39mif\u001b[39;00m out \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrange:\n\u001b[1;32m    698\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/odl/tomo/operators/ray_trafo.py:363\u001b[0m, in \u001b[0;36mRayTransform.adjoint.<locals>.RayBackProjection._call\u001b[0;34m(self, x, out, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, x, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    343\u001b[0m     \u001b[39m\"\"\"Backprojection.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[1;32m    345\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39m        of `RayProjection`.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mreturn\u001b[39;00m ray_trafo\u001b[39m.\u001b[39;49mget_impl(\n\u001b[1;32m    364\u001b[0m         ray_trafo\u001b[39m.\u001b[39;49muse_cache\n\u001b[1;32m    365\u001b[0m     )\u001b[39m.\u001b[39;49mcall_backward(x, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/odl/tomo/backends/util.py:47\u001b[0m, in \u001b[0;36m_add_default_complex_impl.<locals>.wrapper\u001b[0;34m(self, x, out, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, x, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvol_space\u001b[39m.\u001b[39mis_real \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_space\u001b[39m.\u001b[39mis_real:\n\u001b[0;32m---> 47\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, x, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     48\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvol_space\u001b[39m.\u001b[39mis_complex \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_space\u001b[39m.\u001b[39mis_complex:\n\u001b[1;32m     49\u001b[0m         result_parts \u001b[39m=\u001b[39m [\n\u001b[1;32m     50\u001b[0m             fn(\u001b[39mself\u001b[39m, x\u001b[39m.\u001b[39mreal, \u001b[39mgetattr\u001b[39m(out, \u001b[39m'\u001b[39m\u001b[39mreal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs),\n\u001b[1;32m     51\u001b[0m             fn(\u001b[39mself\u001b[39m, x\u001b[39m.\u001b[39mimag, \u001b[39mgetattr\u001b[39m(out, \u001b[39m'\u001b[39m\u001b[39mimag\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     52\u001b[0m         ]\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/odl/tomo/backends/astra_cuda.py:251\u001b[0m, in \u001b[0;36mAstraCudaImpl.call_backward\u001b[0;34m(self, x, out, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m@_add_default_complex_impl\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_backward\u001b[39m(\u001b[39mself\u001b[39m, x, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_backward_real(x, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/odl/tomo/backends/astra_cuda.py:292\u001b[0m, in \u001b[0;36mAstraCudaImpl._call_backward_real\u001b[0;34m(self, proj_data, out, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     astra\u001b[39m.\u001b[39mdata3d\u001b[39m.\u001b[39mstore(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msino_id, swapped_proj_data)\n\u001b[1;32m    291\u001b[0m \u001b[39m# Run algorithm\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m astra\u001b[39m.\u001b[39;49malgorithm\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malgo_backward_id)\n\u001b[1;32m    294\u001b[0m \u001b[39m# Copy result to CPU memory\u001b[39;00m\n\u001b[1;32m    295\u001b[0m out[:] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvol_array\n",
      "File \u001b[0;32m~/anaconda3/envs/odl_torch/lib/python3.9/site-packages/astra/algorithm.py:47\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(i, iterations)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(i, iterations\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     39\u001b[0m     \u001b[39m\"\"\"Run an algorithm.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m    :param i: ID of object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m a\u001b[39m.\u001b[39;49mrun(i,iterations)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.data import get_htc2022_train_phantoms, get_kits_train_phantoms, get_synthetic_htc_phantoms\n",
    "from utils.polynomials import Legendre, Chebyshev\n",
    "from geometries import FlatFanBeamGeometry, DEVICE, HTC2022_GEOMETRY, ParallelGeometry\n",
    "from geometries.geometry_base import naive_sino_filling, mark_cyclic\n",
    "from models.fbps import AdaptiveFBP as AFBP\n",
    "from models.FNOBPs.fnobp import FNO_BP\n",
    "from models.SerieBPs.series_bp1 import Series_BP\n",
    "from models.modelbase import plot_model_progress\n",
    "from statistics import mean\n",
    "\n",
    "ar = 0.25 #angle ratio of full 360 deg scan\n",
    "PHANTOM_DATA = torch.concat([HTC_TRAIN_PHANTOMS, get_synthetic_htc_phantoms()])\n",
    "geometry = HTC2022_GEOMETRY\n",
    "# PHANTOM_DATA = get_kits_train_phantoms()\n",
    "# geometry = FlatFanBeamGeometry(720, 560, 410.66, 543.74, 112, [-40,40, -40, 40], [256, 256])\n",
    "# geometry = FlatFanBeamGeometry(1800, 300, 1.5, 3.0, 4.0, [-1,1,-1,1], [256, 256])\n",
    "# geometry = ParallelGeometry(1800, 300, [-1,1,-1,1], [256, 256])\n",
    "N_known_angles = int(geometry.n_projections*ar)\n",
    "N_angles_out = int(geometry.n_projections*0.6) #can be 0.5 if parallel beam\n",
    "SINO_DATA = geometry.project_forward(PHANTOM_DATA)\n",
    "print(SINO_DATA.dtype, SINO_DATA.device, SINO_DATA.shape)\n",
    "\n",
    "model = Series_BP(geometry, ar, 120, 60, Legendre.key)\n",
    "\n",
    "dataset = TensorDataset(SINO_DATA, PHANTOM_DATA)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "mse_fn = lambda diff : torch.mean(diff**2)\n",
    "n_epochs = 300\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses, batch_sino_losses, batch_recon_losses = [], [], []\n",
    "    for sino_batch, phantom_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_ind = 0 #torch.randint(0, geometry.n_projections, (1,)).item()\n",
    "        known_beta_bool = torch.zeros(geometry.n_projections, device=DEVICE, dtype=bool)\n",
    "        out_beta_bool = known_beta_bool.clone()\n",
    "        mark_cyclic(known_beta_bool, start_ind, (start_ind+N_known_angles)%geometry.n_projections)#known_beta_bool is True at angles where sinogram is meassured and false otherwise\n",
    "        mark_cyclic(out_beta_bool, start_ind, (start_ind+N_angles_out)%geometry.n_projections)\n",
    "        la_sinos = sino_batch * 0 #limited angle sinograms\n",
    "        la_sinos[:, known_beta_bool] = sino_batch[:, known_beta_bool]\n",
    "\n",
    "        filtered = model.get_extrapolated_filtered_sinos(la_sinos, known_beta_bool)\n",
    "        gt_filtered = geometry.inverse_fourier_transform(geometry.fourier_transform(sino_batch*geometry.jacobian_det)*geometry.ram_lak_filter())\n",
    "        loss_sino_domain = mse_fn(gt_filtered-filtered)\n",
    "\n",
    "        recons = F.relu(geometry.project_backward(filtered/2)) #sinogram covers 360deg  - double coverage\n",
    "        loss_recon_domain = mse_fn(phantom_batch - recons)\n",
    "\n",
    "        loss = loss_recon_domain + loss_sino_domain*0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().item())\n",
    "        batch_sino_losses.append(loss_sino_domain.cpu().item())\n",
    "        batch_recon_losses.append(loss_recon_domain.cpu().item())\n",
    "    \n",
    "    print(\"Epoch:\", epoch+1, \"loss is:\", mean(batch_losses), \"sino loss is:\", mean(batch_sino_losses), \"recon loss is:\", mean(batch_recon_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Series_BP\n",
      "sinogram mse: tensor(2913.8262, device='cuda:0')\n",
      "filterd sinogram mse:  tensor(2473.3689, device='cuda:0')\n",
      "reconstruction mse:  tensor(0.9016, device='cuda:0', dtype=torch.float64)\n",
      "========================================\n",
      "FBP\n",
      "sinogram mse: tensor(1245.4938, device='cuda:0')\n",
      "filterd sinogram mse:  tensor(2484.3323, device='cuda:0')\n",
      "reconstruction mse:  tensor(0.1939, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from models.fbps import FBP\n",
    "fbp = FBP(model.geometry)\n",
    "#Clear previous plots\n",
    "for i in plt.get_fignums():\n",
    "    plt.figure(i)\n",
    "    plt.close()\n",
    "\n",
    "VALIDATION_SINOS = geometry.project_forward(VALIDATION_PHANTOMS)\n",
    "zero_cropped_validation_sinos, known_region = geometry.reflect_fill_sinos(*geometry.zero_cropp_sinos(VALIDATION_SINOS, ar, 0))\n",
    "angles = torch.zeros(geometry.n_projections, dtype=bool, device=DEVICE)\n",
    "known_angles = mark_cyclic(angles.clone(), 0, N_known_angles)\n",
    "out_angles = mark_cyclic(angles.clone(), 0, N_angles_out)\n",
    "# zero_cropped_sinos = naive_sino_filling(zero_cropped_sinos, (~known_region).sum(dim=-1) == 0)\n",
    "disp_ind = 0\n",
    "plot_model_progress(model, VALIDATION_SINOS, known_angles, out_angles, VALIDATION_PHANTOMS, disp_ind=disp_ind)\n",
    "plot_model_progress(fbp, VALIDATION_SINOS, known_angles, out_angles, VALIDATION_PHANTOMS, disp_ind=disp_ind)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full angle fbp recons mse\n",
    "recons = geometry.fbp_reconstruct(SINO_DATA)\n",
    "print(torch.mean((recons-PHANTOM_DATA)**2).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to /home/emastr/deep-limited-angle/KEX---CT-reconstruction/data/models/fno_bp_fanbeamkits_ar0.25.pt\n"
     ]
    }
   ],
   "source": [
    "#Save model\n",
    "modelname = \"fno_bp_fanbeamkits_ar0.25\"\n",
    "from models.modelbase import save_model_checkpoint\n",
    "save_path = GIT_ROOT / \"data\" / \"models\" / (modelname + \".pt\")\n",
    "save_model_checkpoint(model, optimizer, loss, ar, save_path)\n",
    "print(\"model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odl_torch",
   "language": "python",
   "name": "odl_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
